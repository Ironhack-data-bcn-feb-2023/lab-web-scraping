{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erik Bernhardsson (erikbern)\n",
      "Hiroshiba (Hiroshiba)\n",
      "Yair Morgenstern (yairm210)\n",
      "Tom Payne (twpayne)\n",
      "Hao Wu (swuecho)\n",
      "Harrison Chase (hwchase17)\n",
      "Matthias Fey (rusty1s)\n",
      "sinclairzx81 (sinclairzx81)\n",
      "Vectorized (Vectorized)\n",
      "Nuno Campos (nfcampos)\n",
      "Alessandro Ros (aler9)\n",
      "Juan Font (juanfont)\n",
      "Etienne BAUDOUX (veler)\n",
      "Sander Verweij (sverweij)\n",
      "Michael Lynch (mtlynch)\n",
      "triple Mu (triple-Mu)\n",
      "Oleh Dokuka (OlegDokuka)\n",
      "Dongdong Tian (seisman)\n",
      "vector (NewByVector)\n",
      "Hadley Wickham (hadley)\n",
      "Ana Hobden (Hoverbear)\n",
      "boojack (boojack)\n",
      "HeYunfei (hyf0)\n",
      "Priyankar Pal (priyankarpal)\n",
      "Charlie Marsh (charliermarsh)\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "div= [i for i in soup.find_all(\"h1\", {\"class\":\"h3 lh-condensed\"})]\n",
    "results= []\n",
    "\n",
    "for tag in div:\n",
    "    a_tag= tag.find(\"a\")\n",
    "    developer_name= a_tag.text.strip()\n",
    "    developer_nickname= a_tag.get(\"href\").replace(\"/\",\"\")\n",
    "    results.append(f\"{developer_name} ({developer_nickname})\")\n",
    "\n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vinta (awesome-python)\n",
      "sdatkinson (neural-amp-modeler)\n",
      "baaivision (Painter)\n",
      "nomic-ai (gpt4all-ui)\n",
      "Winfredy (SadTalker)\n",
      "oobabooga (text-generation-webui)\n",
      "biobootloader (wolverine)\n",
      "chroma-core (chroma)\n",
      "IDEA-Research (GroundingDINO)\n",
      "THUDM (ChatGLM-6B)\n",
      "TabbyML (tabby)\n",
      "liujing04 (Retrieval-based-Voice-Conversion-WebUI)\n",
      "rondinellimorais (facial-expression-recognition)\n",
      "Torantulino (Auto-GPT)\n",
      "sdatkinson (NeuralAmpModelerPlugin)\n",
      "abetlen (llama-cpp-python)\n",
      "rokstrnisa (Robo-GPT)\n",
      "erikbern (ann-benchmarks)\n",
      "jackfrued (Python-100-Days)\n",
      "hwchase17 (langchain)\n",
      "AUTOMATIC1111 (stable-diffusion-webui)\n",
      "TheAlgorithms (Python)\n",
      "whitead (paper-qa)\n",
      "imClumsyPanda (langchain-ChatGLM)\n",
      "fauxpilot (fauxpilot)\n"
     ]
    }
   ],
   "source": [
    "div= [i for i in soup.find_all(\"h1\", {\"class\":\"h3 lh-condensed\"})]\n",
    "results= []\n",
    "\n",
    "for tag in div:\n",
    "    a_tag= tag.find(\"a\")\n",
    "    href_tag= a_tag.get(\"href\")\n",
    "    elements= href_tag.split(\"/\")\n",
    "    del elements[0]\n",
    "    user= elements[0]\n",
    "    repository_name= elements[1]\n",
    "    results.append(f\"{user} ({repository_name})\")\n",
    "        \n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_1946.JPG\n",
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_1942_signature.svg\n",
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_envelope_ca._1921.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Trolley_Troubles_poster.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Steamboat-willie.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_1935.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg\n",
      "https://commons.wikimedia.org/wiki/File:Disney_drawing_goofy.jpg\n",
      "https://commons.wikimedia.org/wiki/File:WaltDisneyplansDisneylandDec1954.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Walt_disney_portrait_right.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Walt_Disney_Grave.JPG\n",
      "https://commons.wikimedia.org/wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg\n",
      "https://commons.wikimedia.org/wiki/File:DisneySchiphol1951.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Disney1968.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Disney_Oscar_1953_(cropped).jpg\n",
      "https://commons.wikimedia.org/wiki/File:Disneyland_Resort_logo.svg\n",
      "https://commons.wikimedia.org/wiki/File:Animation_disc.svg\n",
      "https://commons.wikimedia.org/wiki/File:Magic_Kingdom_castle.jpg\n",
      "https://commons.wikimedia.org/wiki/File:Blank_television_set.svg\n"
     ]
    }
   ],
   "source": [
    "div= [i for i in soup.find_all(\"a\", {\"class\":\"image\"})]\n",
    "results= []\n",
    "\n",
    "for tag in div:\n",
    "    href_tag= tag.get(\"href\")\n",
    "    results.append(f\"https://commons.wikimedia.org{href_tag}\")\n",
    "        \n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Pythonidae\n",
      "https://en.wikipedia.org/wiki/Python_(genus)\n",
      "https://en.wikipedia.org/wiki/Python_(mythology)\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://en.wikipedia.org/wiki/CMU_Common_Lisp\n",
      "https://en.wikipedia.org/wiki/PERQ#PERQ_3\n",
      "https://en.wikipedia.org/wiki/Python_of_Aenus\n",
      "https://en.wikipedia.org/wiki/Python_(painter)\n",
      "https://en.wikipedia.org/wiki/Python_of_Byzantium\n",
      "https://en.wikipedia.org/wiki/Python_of_Catana\n",
      "https://en.wikipedia.org/wiki/Python_Anghelo\n",
      "https://en.wikipedia.org/wiki/Python_(Efteling)\n",
      "https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "https://en.wikipedia.org/wiki/Python_(automobile_maker)\n",
      "https://en.wikipedia.org/wiki/Python_(Ford_prototype)\n",
      "https://en.wikipedia.org/wiki/Python_(missile)\n",
      "https://en.wikipedia.org/wiki/Python_(nuclear_primary)\n",
      "https://en.wikipedia.org/wiki/Colt_Python\n",
      "https://en.wikipedia.org/wiki/Python_(codename)\n",
      "https://en.wikipedia.org/wiki/Python_(film)\n",
      "https://en.wikipedia.org/wiki/Monty_Python\n",
      "https://en.wikipedia.org/wiki/Python_(Monty)_Pictures\n",
      "https://en.wikipedia.org/wiki/Timon_of_Phlius\n",
      "https://en.wikipedia.org/wiki/Pyton\n",
      "https://en.wikipedia.org/wiki/Pithon\n"
     ]
    }
   ],
   "source": [
    "div= soup.find(\"div\", {\"class\":\"mw-parser-output\"})\n",
    "results= []\n",
    "a_tag= div.find_all(\"a\")\n",
    "\n",
    "href= []\n",
    "for i in a_tag:\n",
    "    if i.get(\"href\").startswith(\"/wiki\"):\n",
    "        href= i.get(\"href\")\n",
    "        results.append(f\"https://en.wikipedia.org{href}\")\n",
    "\n",
    "results.pop() # no forma parte de la lista de resultados de búsqueda arbitraria, es otro elemento del div\n",
    "results.pop() # no forma parte de la lista de resultados de búsqueda arbitraria, es otro elemento del div\n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Titles that have changed in the United States Code since its last release point is: 53\n"
     ]
    }
   ],
   "source": [
    "divs= soup.find_all(\"div\", {\"class\":\"usctitle\"})\n",
    "results= []\n",
    "\n",
    "for i in divs:\n",
    "    if i.text.strip().startswith(\"Title\"):\n",
    "        results.append(i.text.strip())\n",
    "\n",
    "print(f\"The number of Titles that have changed in the United States Code since its last release point is: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>15:13:56.6</td>\n",
       "      <td>38.16 N</td>\n",
       "      <td>38.55 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>15:11:44.0</td>\n",
       "      <td>8.01 S</td>\n",
       "      <td>116.03 E</td>\n",
       "      <td>LOMBOK REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>15:10:07.0</td>\n",
       "      <td>2.93 S</td>\n",
       "      <td>128.76 E</td>\n",
       "      <td>CERAM SEA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>15:00:12.7</td>\n",
       "      <td>37.83 N</td>\n",
       "      <td>36.74 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:56:02.0</td>\n",
       "      <td>9.50 S</td>\n",
       "      <td>115.82 E</td>\n",
       "      <td>SOUTH OF BALI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:55:00.0</td>\n",
       "      <td>24.16 S</td>\n",
       "      <td>67.55 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:32:45.5</td>\n",
       "      <td>47.37 N</td>\n",
       "      <td>6.91 E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:27:48.0</td>\n",
       "      <td>21.64 S</td>\n",
       "      <td>68.58 W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:22:45.7</td>\n",
       "      <td>38.73 N</td>\n",
       "      <td>39.69 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:12:28.1</td>\n",
       "      <td>17.94 N</td>\n",
       "      <td>66.96 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>14:00:57.1</td>\n",
       "      <td>41.13 N</td>\n",
       "      <td>20.31 E</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:53:50.8</td>\n",
       "      <td>38.56 N</td>\n",
       "      <td>122.31 W</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:50:18.6</td>\n",
       "      <td>41.11 N</td>\n",
       "      <td>20.24 E</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:46:51.5</td>\n",
       "      <td>37.99 N</td>\n",
       "      <td>36.60 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:39:15.2</td>\n",
       "      <td>39.04 N</td>\n",
       "      <td>40.46 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:34:11.9</td>\n",
       "      <td>39.41 N</td>\n",
       "      <td>44.44 E</td>\n",
       "      <td>TURKEY-IRAN BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:34:04.0</td>\n",
       "      <td>19.67 N</td>\n",
       "      <td>71.22 W</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:21:28.6</td>\n",
       "      <td>37.79 N</td>\n",
       "      <td>36.27 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:11:52.0</td>\n",
       "      <td>8.14 S</td>\n",
       "      <td>120.27 E</td>\n",
       "      <td>FLORES REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>13:08:08.0</td>\n",
       "      <td>6.35 N</td>\n",
       "      <td>126.47 E</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date        time latitude longitude                region name\n",
       "0   2023-04-11  15:13:56.6  38.16 N   38.55 E             EASTERN TURKEY\n",
       "1   2023-04-11  15:11:44.0   8.01 S  116.03 E   LOMBOK REGION, INDONESIA\n",
       "2   2023-04-11  15:10:07.0   2.93 S  128.76 E       CERAM SEA, INDONESIA\n",
       "3   2023-04-11  15:00:12.7  37.83 N   36.74 E             CENTRAL TURKEY\n",
       "4   2023-04-11  14:56:02.0   9.50 S  115.82 E   SOUTH OF BALI, INDONESIA\n",
       "5   2023-04-11  14:55:00.0  24.16 S   67.55 W           SALTA, ARGENTINA\n",
       "6   2023-04-11  14:32:45.5  47.37 N    6.91 E                SWITZERLAND\n",
       "7   2023-04-11  14:27:48.0  21.64 S   68.58 W         ANTOFAGASTA, CHILE\n",
       "8   2023-04-11  14:22:45.7  38.73 N   39.69 E             EASTERN TURKEY\n",
       "9   2023-04-11  14:12:28.1  17.94 N   66.96 W         PUERTO RICO REGION\n",
       "10  2023-04-11  14:00:57.1  41.13 N   20.31 E                    ALBANIA\n",
       "11  2023-04-11  13:53:50.8  38.56 N  122.31 W        NORTHERN CALIFORNIA\n",
       "12  2023-04-11  13:50:18.6  41.11 N   20.24 E                    ALBANIA\n",
       "13  2023-04-11  13:46:51.5  37.99 N   36.60 E             CENTRAL TURKEY\n",
       "14  2023-04-11  13:39:15.2  39.04 N   40.46 E             EASTERN TURKEY\n",
       "15  2023-04-11  13:34:11.9  39.41 N   44.44 E  TURKEY-IRAN BORDER REGION\n",
       "16  2023-04-11  13:34:04.0  19.67 N   71.22 W         DOMINICAN REPUBLIC\n",
       "17  2023-04-11  13:21:28.6  37.79 N   36.27 E             CENTRAL TURKEY\n",
       "18  2023-04-11  13:11:52.0   8.14 S  120.27 E   FLORES REGION, INDONESIA\n",
       "19  2023-04-11  13:08:08.0   6.35 N  126.47 E      MINDANAO, PHILIPPINES"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div= soup.find(\"tbody\", {\"id\":\"tbody\"})\n",
    "\n",
    "# limitar resultados a 20\n",
    "div20= []\n",
    "limit= 20\n",
    "for i in div:\n",
    "    if limit > 0 and i != \"\\n\":\n",
    "        div20.append(i)\n",
    "        limit-=1\n",
    "\n",
    "# dict para crear dataframe\n",
    "data= {\n",
    "    \"date\":[],\n",
    "    \"time\":[],\n",
    "    \"latitude\":[],\n",
    "    \"longitude\":[],\n",
    "    \"region name\":[],\n",
    "}\n",
    "\n",
    "for i in div20:\n",
    "    a_tag= i.find(\"td\", {\"class\":\"tabev6\"}).find(\"a\").text\n",
    "\n",
    "    # date\n",
    "    date= a_tag[:10]\n",
    "    data[\"date\"].append(date)\n",
    "\n",
    "    # time\n",
    "    time= a_tag[-10:]\n",
    "    data[\"time\"].append(time)\n",
    "\n",
    "    # latitude\n",
    "    tabev1= i.find_all(\"td\", {\"class\":\"tabev1\"}) # hay 2 td con misma class, 1 para latitude y 1 para longitude\n",
    "    tabev2= i.find_all(\"td\", {\"class\":\"tabev2\"}) # hay 3 td con misma class, 1 para latitude, 1 para longitude y 1 para \"Mag\" que voy a ignorar\n",
    "    pre_latitude= tabev1[0].text\n",
    "    latitude_number= \"\"\n",
    "    for c in pre_latitude:\n",
    "        if c.isdigit() or c == \".\":\n",
    "            latitude_number+= c\n",
    "    latitude_word= tabev2[0].text[0]\n",
    "    latitude= latitude_number + \" \" + latitude_word\n",
    "    data[\"latitude\"].append(latitude)\n",
    "\n",
    "    # longitude\n",
    "    pre_longitude= tabev1[1].text\n",
    "    longitude_number= \"\"\n",
    "    for c in pre_longitude:\n",
    "        if c.isdigit() or c == \".\":\n",
    "            longitude_number+= c\n",
    "    longitude_word= tabev2[1].text[0]\n",
    "    longitude= longitude_number + \" \" + longitude_word\n",
    "    data[\"longitude\"].append(longitude)\n",
    "\n",
    "    # region_name\n",
    "    region_name= i.find(\"td\", {\"class\":\"tb_region\"}).text[1:]\n",
    "    data[\"region name\"].append(region_name)\n",
    "\n",
    "# dataframe\n",
    "df= pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "\n",
    "# no puedo acceder a esta web. parece que ya no existe(?) ni siquiera accediendo desde su LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista ordenada por ranking views/day, tal como se reciben los elementos:\n",
      "1.\tEnglish\t\t-\t6 638 000+\n",
      "2.\tRusian\t\t-\t1 905 000+\n",
      "3.\tEspañol\t\t-\t1 851 000+\n",
      "4.\tJapanese\t-\t1 368 000+\n",
      "5.\tDeutsch\t\t-\t2 788 000+\n",
      "6.\tFrançais\t-\t2 510 000+\n",
      "7.\tItaliano\t-\t1 805 000+\n",
      "8.\tZhongwen\t-\t1 344 000+\n",
      "9.\tFarsi\t\t-\t957 000+\n",
      "10.\tPortuguês\t-\t1 103 000+\n",
      "\n",
      "Lista ordenada por orden de los div en la web:\n",
      "1.\tEspañol\t\t-\t1 851 000+\n",
      "2.\tEnglish\t\t-\t6 638 000+\n",
      "3.\tRusian\t\t-\t1 905 000+\n",
      "4.\tJapanese\t-\t1 368 000+\n",
      "5.\tDeutsch\t\t-\t2 788 000+\n",
      "6.\tFrançais\t-\t2 510 000+\n",
      "7.\tItaliano\t-\t1 805 000+\n",
      "8.\tZhongwen\t-\t1 344 000+\n",
      "9.\tFarsi\t\t-\t957 000+\n",
      "10.\tPortuguês\t-\t1 103 000+\n"
     ]
    }
   ],
   "source": [
    "from bs4 import element\n",
    "\n",
    "div= soup.find(\"div\", {\"class\":\"central-featured\"})\n",
    "\n",
    "# limpiar los bs4.element recibidos en el div para coger sólo los Tag\n",
    "c_div= []\n",
    "for i in div:\n",
    "    if isinstance(i, element.Tag):\n",
    "        c_div.append(i)\n",
    "\n",
    "ranking= {}\n",
    "\n",
    "for i in c_div:\n",
    "    # country\n",
    "    language= i.find(\"strong\").text\n",
    "    if language == \"Русский\":\n",
    "        language= \"Rusian\"\n",
    "    if language == \"日本語\":\n",
    "        language= \"Japanese\"\n",
    "    if language == \"中文\":\n",
    "        language= \"Zhongwen\"\n",
    "    if language == \"فارسی\":\n",
    "        language= \"Farsi\"\n",
    "    # related articles\n",
    "    rel_artc= i.find(\"bdi\", {\"dir\":\"ltr\"}).text\n",
    "    \n",
    "    ranking[language]= rel_artc\n",
    "\n",
    "print(\"Lista ordenada por ranking views/day, tal como se reciben los elementos:\")\n",
    "n=1\n",
    "for k,v in ranking.items():\n",
    "    if len(k) < 8:\n",
    "        print(f\"{n}.\\t{k}\\t\\t-\\t{v}\")\n",
    "    else:\n",
    "        print(f\"{n}.\\t{k}\\t-\\t{v}\")\n",
    "    n+=1\n",
    "\n",
    "print(\"\\nLista ordenada por orden de los div en la web:\")\n",
    "\n",
    "order_dict= {\n",
    "    1:\"Español\",\n",
    "    2:\"English\",\n",
    "    3:\"Rusian\",\n",
    "    4:\"Japanese\",\n",
    "    5:\"Deutsch\",\n",
    "    6:\"Français\",\n",
    "    7:\"Italiano\",\n",
    "    8:\"Zhongwen\",\n",
    "    9:\"Farsi\",\n",
    "    10:\"Português\",\n",
    "}\n",
    "\n",
    "# indico que por cada key/value en dict(order_dict) quiero imprimir k/v de order_dict seguido del correspondiente valor en el dict(ranking)\n",
    "for k,v in order_dict.items():\n",
    "    if v in ranking.keys():\n",
    "        if len(v) < 8:\n",
    "            print(f\"{k}.\\t{v}\\t\\t-\\t{ranking[v]}\")\n",
    "        else:\n",
    "            print(f\"{k}.\\t{v}\\t-\\t{ranking[v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business and economy\n",
      "Crime and justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns and cities\n",
      "Transport\n",
      "Digital service performance\n",
      "Government reference data\n"
     ]
    }
   ],
   "source": [
    "ul= soup.find(\"ul\", {\"class\":\"govuk-list dgu-topics__list\"}).find_all(\"li\")\n",
    "\n",
    "the_list= []\n",
    "\n",
    "for i in ul:\n",
    "    dataset_kind= i.find(\"a\").text\n",
    "    the_list.append(dataset_kind)\n",
    "\n",
    "for i in the_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers (millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>939</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>485</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>380</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>345</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>236</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russian</td>\n",
       "      <td>147</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>123</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>86.1</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Language Native speakers (millions) Language family        Branch\n",
       "1   Mandarin Chinese                        939    Sino-Tibetan       Sinitic\n",
       "2            Spanish                        485   Indo-European       Romance\n",
       "3            English                        380   Indo-European      Germanic\n",
       "4              Hindi                        345   Indo-European    Indo-Aryan\n",
       "5         Portuguese                        236   Indo-European       Romance\n",
       "6            Bengali                        234   Indo-European    Indo-Aryan\n",
       "7            Russian                        147   Indo-European  Balto-Slavic\n",
       "8           Japanese                        123         Japonic      Japanese\n",
       "9        Yue Chinese                       86.1    Sino-Tibetan       Sinitic\n",
       "10        Vietnamese                       85.0   Austroasiatic        Vietic"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= {\n",
    "    \"Language\":[],\n",
    "    \"Native speakers (millions)\":[],\n",
    "    \"Language family\":[],\n",
    "    \"Branch\":[],\n",
    "}\n",
    "\n",
    "tbody= soup.find(\"tbody\")\n",
    "trs= tbody.find_all(\"tr\")\n",
    "del trs[0] # pese a especificarle que sólo quiero <tbody>, el <tr> de <thead> se ha acoplado a la fiesta\n",
    "\n",
    "for i in trs:\n",
    "    tds= i.find_all(\"td\")\n",
    "    language= tds[0].find(\"a\").getText()\n",
    "    data[\"Language\"].append(language)\n",
    "    native_speakers= tds[1].getText()[:-1]\n",
    "    data[\"Native speakers (millions)\"].append(native_speakers)\n",
    "    language_family= tds[2].find(\"a\").getText()\n",
    "    data[\"Language family\"].append(language_family)\n",
    "    branch= tds[3].getText()[:-1]\n",
    "    data[\"Branch\"].append(branch)\n",
    "\n",
    "index= list(range(1, len(data[\"Language\"])+1))\n",
    "df= pd.DataFrame(data, index=index)\n",
    "top10= df.head(10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "res= requests.get(url)\n",
    "soup= BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie name</th>\n",
       "      <th>Initial release</th>\n",
       "      <th>Director name</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El padrino (parte II)</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>El gigante de hierro</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>Brad Bird</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Criadas y señoras</td>\n",
       "      <td>(2011)</td>\n",
       "      <td>Tate Taylor</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Aladdín</td>\n",
       "      <td>(1992)</td>\n",
       "      <td>Ron Clements</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Dersu Uzala (El cazador)</td>\n",
       "      <td>(1975)</td>\n",
       "      <td>Akira Kurosawa</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Bailando con lobos</td>\n",
       "      <td>(1990)</td>\n",
       "      <td>Kevin Costner</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Movie name Initial release         Director name Stars\n",
       "1             Cadena perpetua          (1994)        Frank Darabont  9.2 \n",
       "2                  El padrino          (1972)  Francis Ford Coppola  9.2 \n",
       "3         El caballero oscuro          (2008)     Christopher Nolan  9.0 \n",
       "4       El padrino (parte II)          (1974)  Francis Ford Coppola  9.0 \n",
       "5       12 hombres sin piedad          (1957)          Sidney Lumet  9.0 \n",
       "..                        ...             ...                   ...   ...\n",
       "246      El gigante de hierro          (1999)             Brad Bird  8.0 \n",
       "247         Criadas y señoras          (2011)           Tate Taylor  8.0 \n",
       "248                   Aladdín          (1992)          Ron Clements  8.0 \n",
       "249  Dersu Uzala (El cazador)          (1975)        Akira Kurosawa  8.0 \n",
       "250        Bailando con lobos          (1990)         Kevin Costner  8.0 \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= {\n",
    "    \"Movie name\":[],\n",
    "    \"Initial release\":[],\n",
    "    \"Director name\":[],\n",
    "    \"Stars\":[],\n",
    "}\n",
    "\n",
    "movies= soup.find(\"tbody\", {\"class\":\"lister-list\"})\n",
    "movie_row= movies.find_all(\"tr\")\n",
    "for i in movie_row:\n",
    "    movie_name= i.find(\"td\", {\"class\":\"titleColumn\"}).find(\"a\").getText()\n",
    "    data[\"Movie name\"].append(movie_name)\n",
    "    initial_release= i.find(\"td\", {\"class\":\"titleColumn\"}).find(\"span\", {\"class\":\"secondaryInfo\"}).getText()\n",
    "    data[\"Initial release\"].append(initial_release)\n",
    "    director_name= i.find(\"td\", {\"class\":\"titleColumn\"}).find(\"a\").get(\"title\").split(\",\")[0][:-7]\n",
    "    data[\"Director name\"].append(director_name)\n",
    "    stars= i.find(\"td\", {\"class\":\"ratingColumn imdbRating\"}).find(\"strong\").get(\"title\")[:4]\n",
    "    data[\"Stars\"].append(stars)\n",
    "    \n",
    "index= list(range(1, len(data[\"Movie name\"])+1))\n",
    "df= pd.DataFrame(data, index=index)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "706ef8b4fdc2a6062896a87ccd1e59e0430ad7969de34dd64ca4fc4ab8043f48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
