{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<a data-hydro-click=\"{&quot;event_type&quot;:&quot;explore.click&quot;,&quot;payload&quot;:{&quot;click_context&quot;:&quot;TRENDING_DEVELOPERS_PAGE&quot;,&quot;click_target&quot;:&quot;OWNER&quot;,&quot;click_visual_representation&quot;:&quot;TRENDING_DEVELOPER&quot;,&quot;actor_id&quot;:null,&quot;record_id&quot;:216188,&quot;originating_url&quot;:&quot;https://github.com/trending/developers&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"896a148d5d4218b3eaa307d034cece6923d350afb64b058e74d9061d6ca47324\" href=\"/jdxcode\" data-view-component=\"true\" class=\"Link--secondary\">jdxcode\n",
    "#</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n              jdxcode\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "soup.find_all(\"a\", attrs={\"class\":\"Link--secondary\"})[30].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Himself65',\n",
       " 'dessalines',\n",
       " 'hwchase17',\n",
       " 'Narsil',\n",
       " 'jdxcode',\n",
       " 'wader',\n",
       " 'jerryjliu',\n",
       " 'hathach',\n",
       " 'nfcampos',\n",
       " 'chrisbanes',\n",
       " 'marcusolsson',\n",
       " 'zkochan',\n",
       " 'matklad',\n",
       " 'aler9',\n",
       " 'tobymao',\n",
       " 'biodranik',\n",
       " 'fonsp',\n",
       " 'syuilo',\n",
       " 'azure-sdk',\n",
       " 'didierganthier']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = soup.find_all('a', attrs={'class':'Link--secondary'})\n",
    "trending_dev = []\n",
    "#from 26 onwards as the prior data is not relevant and is tagged with the same class attribute\n",
    "for i in all_data[26:]:\n",
    "    if i.get('href').startswith('/'):\n",
    "        trending_dev.append(i.get('href').replace('/',''))\n",
    "        \n",
    "trending_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req2 = requests.get(url2)\n",
    "soup2 = BeautifulSoup(req2.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/modelscope/modelscope',\n",
       " '/Stability-AI/stablediffusion',\n",
       " '/GaiZhenbiao/ChuanhuChatGPT',\n",
       " '/home-assistant/core',\n",
       " '/AUTOMATIC1111/stable-diffusion-webui',\n",
       " '/34j/so-vits-svc-fork',\n",
       " '/acheong08/EdgeGPT',\n",
       " '/bregman-arie/devops-exercises',\n",
       " '/karpathy/nanoGPT',\n",
       " '/TencentARC/GFPGAN',\n",
       " '/comfyanonymous/ComfyUI',\n",
       " '/hwchase17/chat-langchain',\n",
       " '/lucidrains/PaLM-rlhf-pytorch',\n",
       " '/Akegarasu/ChatGLM-webui',\n",
       " '/neonbjb/tortoise-tts',\n",
       " '/zhayujie/chatgpt-on-wechat',\n",
       " '/acheong08/ChatGPT',\n",
       " '/Zero6992/chatGPT-discord-bot',\n",
       " '/pytube/pytube',\n",
       " '/pointnetwork/point-alpaca',\n",
       " '/iperov/DeepFaceLab',\n",
       " '/hwchase17/langchain',\n",
       " '/crowsonkb/k-diffusion',\n",
       " '/karfly/chatgpt_telegram_bot',\n",
       " '/openai/whisper']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_repo = soup2.find_all(\"h1\", attrs={\"class\":\"h3 lh-condensed\"})\n",
    "trending_repo = []\n",
    "#find all h1 with the class, inside finde a which contains the links and then href for the repo ref (going directly to a from find all brings too many links)\n",
    "for i in all_repo:\n",
    "    if i.find('a').get('href'):\n",
    "        trending_repo.append(i.find('a').get('href'))\n",
    "        \n",
    "trending_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req3 = requests.get(url3)\n",
    "soup3 = BeautifulSoup(req3.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org//wiki/File:Walt_Disney_1946.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Steamboat-willie.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_1935.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney_drawing_goofy.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_disney_portrait_right.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Grave.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:DisneySchiphol1951.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney1968.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney_Oscar_1953_(cropped).jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disneyland_Resort_logo.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Animation_disc.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Magic_Kingdom_castle.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Blank_television_set.svg']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img = soup3.find_all(\"a\", attrs={\"class\":\"image\"})\n",
    "img_repo = []\n",
    "#find all h1 with the class, inside finde a which contains the links and then href for the repo ref (going directly to a from find all brings too many links)\n",
    "for i in all_img:\n",
    "    if i.get('href'):\n",
    "        img_repo.append(\"https://en.wikipedia.org/\"+i.get('href'))\n",
    "        \n",
    "img_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url4 ='https://en.wikipedia.org/wiki/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req4 = requests.get(url4)\n",
    "soup4 = BeautifulSoup(req4.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-Snakes-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-Computing-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-People-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-Roller_coasters-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-Vehicles-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-Weaponry-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-Other_uses-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-toc-list\" id=\"toc-See_also-sublist\">\n",
      "</ul>\n",
      "something went wrong with:<ul class=\"vector-menu-content-list\"></ul>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(codename)',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Pyton']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tried with: all_links = soup4.find_all(\"div\", attrs={\"class\":\"mw-parser-output\"}) but did not work\n",
    "all_links = soup4.find_all('ul')\n",
    "links_repo = []\n",
    "\n",
    "for i in all_links:\n",
    "    try:\n",
    "        if i.find('a').get('href').startswith('/wiki/Py'):\n",
    "            links_repo.append(i.find('a').get('href'))\n",
    "    except:\n",
    "        print(f\"something went wrong with:{i}\")\n",
    "    \n",
    "links_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req5 = requests.get(url5)\n",
    "soup5 = BeautifulSoup(req5.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 5 - Government Organization and Employees ٭',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 8 - Aliens and Nationality',\n",
       " 'Title 11 - Bankruptcy ٭',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 18 - Crimes and Criminal Procedure ٭',\n",
       " 'Title 22 - Foreign Relations and Intercourse',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 28 - Judiciary and Judicial Procedure ٭',\n",
       " 'Title 34 - Crime Control and Law Enforcement',\n",
       " 'Title 36 - Patriotic and National Observances, Ceremonies, and Organizations ٭',\n",
       " \"Title 38 - Veterans' Benefits ٭\",\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 50 - War and National Defense',\n",
       " 'Title 54 - National Park Service and Related Programs ٭']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_changed = soup5.find_all('div', attrs ={'class': 'usctitlechanged'})\n",
    "changed_repo = []\n",
    "\n",
    "for i in all_changed:\n",
    "    if i.getText().strip():\n",
    "        changed_repo.append(i.getText().strip())\n",
    "        \n",
    "changed_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url6 = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req6 = requests.get(url6)\n",
    "soup6 = BeautifulSoup(req6.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Citizen  Response</th>\n",
       "      <th>Date &amp; Time  UTC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Latitude  degrees</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Longitude  degrees</th>\n",
       "      <th>Depth  km</th>\n",
       "      <th>Mag  [+]</th>\n",
       "      <th>Region name  [+]</th>\n",
       "      <th>Last update  [-]</th>\n",
       "      <th>Unnamed: 12_level_0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  ».1</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  ».2</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  ».1</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  ».1</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "      <th>1  2  3  4  5  6  7  8  9  10  ›  »</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:55:32.0  06min ago</td>\n",
       "      <td>38.43</td>\n",
       "      <td>N</td>\n",
       "      <td>37.34</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:50:40.5  11min ago</td>\n",
       "      <td>37.81</td>\n",
       "      <td>N</td>\n",
       "      <td>36.62</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>M</td>\n",
       "      <td>3.1</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:41:38.0  20min ago</td>\n",
       "      <td>28.00</td>\n",
       "      <td>S</td>\n",
       "      <td>70.70</td>\n",
       "      <td>W</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>2.5</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "      <td>2023-03-21 10:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:40:57.0  21min ago</td>\n",
       "      <td>12.62</td>\n",
       "      <td>N</td>\n",
       "      <td>87.96</td>\n",
       "      <td>W</td>\n",
       "      <td>32</td>\n",
       "      <td>M</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "      <td>2023-03-21 10:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:33:18.6  28min ago</td>\n",
       "      <td>37.50</td>\n",
       "      <td>N</td>\n",
       "      <td>37.18</td>\n",
       "      <td>E</td>\n",
       "      <td>9</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.1</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:17:08.5  44min ago</td>\n",
       "      <td>37.35</td>\n",
       "      <td>N</td>\n",
       "      <td>36.90</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.9</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:13:22.1  48min ago</td>\n",
       "      <td>38.12</td>\n",
       "      <td>N</td>\n",
       "      <td>37.71</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 10:10:59.4  51min ago</td>\n",
       "      <td>44.88</td>\n",
       "      <td>S</td>\n",
       "      <td>167.79</td>\n",
       "      <td>E</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SOUTH ISLAND OF NEW ZEALAND</td>\n",
       "      <td>2023-03-21 10:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-03-21 09:59:15.0  1hr 02min ago</td>\n",
       "      <td>38.38</td>\n",
       "      <td>N</td>\n",
       "      <td>45.25</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NORTHWESTERN IRAN</td>\n",
       "      <td>2023-03-21 10:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 09:50:04.0  1hr 11min ago</td>\n",
       "      <td>21.53</td>\n",
       "      <td>S</td>\n",
       "      <td>68.59</td>\n",
       "      <td>W</td>\n",
       "      <td>127</td>\n",
       "      <td>M</td>\n",
       "      <td>2.6</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>2023-03-21 10:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 09:48:58.0  1hr 13min ago</td>\n",
       "      <td>37.99</td>\n",
       "      <td>N</td>\n",
       "      <td>36.54</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>2.8</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 09:18:50.0  1hr 43min ago</td>\n",
       "      <td>4.68</td>\n",
       "      <td>S</td>\n",
       "      <td>102.63</td>\n",
       "      <td>E</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>3.6</td>\n",
       "      <td>SOUTHERN SUMATRA, INDONESIA</td>\n",
       "      <td>2023-03-21 09:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 09:15:16.0  1hr 46min ago</td>\n",
       "      <td>35.19</td>\n",
       "      <td>N</td>\n",
       "      <td>3.58</td>\n",
       "      <td>W</td>\n",
       "      <td>3</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.5</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "      <td>2023-03-21 09:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 09:13:31.3  1hr 48min ago</td>\n",
       "      <td>56.89</td>\n",
       "      <td>N</td>\n",
       "      <td>158.08</td>\n",
       "      <td>W</td>\n",
       "      <td>10</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ALASKA PENINSULA</td>\n",
       "      <td>2023-03-21 09:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 09:00:48.3  2hr 01min ago</td>\n",
       "      <td>7.65</td>\n",
       "      <td>S</td>\n",
       "      <td>127.51</td>\n",
       "      <td>E</td>\n",
       "      <td>166</td>\n",
       "      <td>mb</td>\n",
       "      <td>4.4</td>\n",
       "      <td>KEPULAUAN BARAT DAYA, INDONESIA</td>\n",
       "      <td>2023-03-21 10:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 08:54:06.1  2hr 07min ago</td>\n",
       "      <td>38.46</td>\n",
       "      <td>N</td>\n",
       "      <td>37.22</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.4</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 08:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-03-21 08:40:49.0  2hr 21min ago</td>\n",
       "      <td>38.02</td>\n",
       "      <td>N</td>\n",
       "      <td>36.37</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.7</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-03-21 08:27:55.3  2hr 34min ago</td>\n",
       "      <td>37.82</td>\n",
       "      <td>N</td>\n",
       "      <td>36.30</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.9</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>2023-03-21 10:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>2023-03-21 08:17:42.3  2hr 44min ago</td>\n",
       "      <td>45.16</td>\n",
       "      <td>N</td>\n",
       "      <td>23.15</td>\n",
       "      <td>E</td>\n",
       "      <td>16</td>\n",
       "      <td>ML</td>\n",
       "      <td>2.7</td>\n",
       "      <td>ROMANIA</td>\n",
       "      <td>2023-03-21 10:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-21 08:15:16.9  2hr 46min ago</td>\n",
       "      <td>32.39</td>\n",
       "      <td>N</td>\n",
       "      <td>115.23</td>\n",
       "      <td>W</td>\n",
       "      <td>8</td>\n",
       "      <td>Ml</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BAJA CALIFORNIA, MEXICO</td>\n",
       "      <td>2023-03-21 08:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Citizen  Response                                        \\\n",
       "   1  2  3  4  5  6  7  8  9  10  ›  » 1  2  3  4  5  6  7  8  9  10  ›  ».1   \n",
       "0                                  NaN                                   NaN   \n",
       "1                                  NaN                                   NaN   \n",
       "2                                  NaN                                   NaN   \n",
       "3                                  NaN                                   NaN   \n",
       "4                                  NaN                                   NaN   \n",
       "5                                  NaN                                   NaN   \n",
       "6                                  NaN                                   NaN   \n",
       "7                                  NaN                                   NaN   \n",
       "8                                    1                                   NaN   \n",
       "9                                  NaN                                   NaN   \n",
       "10                                 NaN                                   NaN   \n",
       "11                                 NaN                                   NaN   \n",
       "12                                 NaN                                   NaN   \n",
       "13                                 NaN                                   NaN   \n",
       "14                                 NaN                                   NaN   \n",
       "15                                 NaN                                   NaN   \n",
       "16                                   2                                   NaN   \n",
       "17                                 NaN                                   NaN   \n",
       "18                                   2                                   NaN   \n",
       "19                                 NaN                                   NaN   \n",
       "\n",
       "                                          \\\n",
       "   1  2  3  4  5  6  7  8  9  10  ›  ».2   \n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "5                                    NaN   \n",
       "6                                    NaN   \n",
       "7                                    NaN   \n",
       "8                                      F   \n",
       "9                                    NaN   \n",
       "10                                   NaN   \n",
       "11                                   NaN   \n",
       "12                                   NaN   \n",
       "13                                   NaN   \n",
       "14                                   NaN   \n",
       "15                                   NaN   \n",
       "16                                     F   \n",
       "17                                     F   \n",
       "18                                     F   \n",
       "19                                   NaN   \n",
       "\n",
       "                        Date & Time  UTC                   Latitude  degrees  \\\n",
       "     1  2  3  4  5  6  7  8  9  10  ›  » 1  2  3  4  5  6  7  8  9  10  ›  »   \n",
       "0       2023-03-21 10:55:32.0  06min ago                               38.43   \n",
       "1       2023-03-21 10:50:40.5  11min ago                               37.81   \n",
       "2       2023-03-21 10:41:38.0  20min ago                               28.00   \n",
       "3       2023-03-21 10:40:57.0  21min ago                               12.62   \n",
       "4       2023-03-21 10:33:18.6  28min ago                               37.50   \n",
       "5       2023-03-21 10:17:08.5  44min ago                               37.35   \n",
       "6       2023-03-21 10:13:22.1  48min ago                               38.12   \n",
       "7       2023-03-21 10:10:59.4  51min ago                               44.88   \n",
       "8   2023-03-21 09:59:15.0  1hr 02min ago                               38.38   \n",
       "9   2023-03-21 09:50:04.0  1hr 11min ago                               21.53   \n",
       "10  2023-03-21 09:48:58.0  1hr 13min ago                               37.99   \n",
       "11  2023-03-21 09:18:50.0  1hr 43min ago                                4.68   \n",
       "12  2023-03-21 09:15:16.0  1hr 46min ago                               35.19   \n",
       "13  2023-03-21 09:13:31.3  1hr 48min ago                               56.89   \n",
       "14  2023-03-21 09:00:48.3  2hr 01min ago                                7.65   \n",
       "15  2023-03-21 08:54:06.1  2hr 07min ago                               38.46   \n",
       "16  2023-03-21 08:40:49.0  2hr 21min ago                               38.02   \n",
       "17  2023-03-21 08:27:55.3  2hr 34min ago                               37.82   \n",
       "18  2023-03-21 08:17:42.3  2hr 44min ago                               45.16   \n",
       "19  2023-03-21 08:15:16.9  2hr 46min ago                               32.39   \n",
       "\n",
       "                                                          Longitude  degrees  \\\n",
       "   1  2  3  4  5  6  7  8  9  10  ›  ».1 1  2  3  4  5  6  7  8  9  10  ›  »   \n",
       "0                                      N                               37.34   \n",
       "1                                      N                               36.62   \n",
       "2                                      S                               70.70   \n",
       "3                                      N                               87.96   \n",
       "4                                      N                               37.18   \n",
       "5                                      N                               36.90   \n",
       "6                                      N                               37.71   \n",
       "7                                      S                              167.79   \n",
       "8                                      N                               45.25   \n",
       "9                                      S                               68.59   \n",
       "10                                     N                               36.54   \n",
       "11                                     S                              102.63   \n",
       "12                                     N                                3.58   \n",
       "13                                     N                              158.08   \n",
       "14                                     S                              127.51   \n",
       "15                                     N                               37.22   \n",
       "16                                     N                               36.37   \n",
       "17                                     N                               36.30   \n",
       "18                                     N                               23.15   \n",
       "19                                     N                              115.23   \n",
       "\n",
       "                                                                   Depth  km  \\\n",
       "   1  2  3  4  5  6  7  8  9  10  ›  ».1 1  2  3  4  5  6  7  8  9  10  ›  »   \n",
       "0                                      E                                   5   \n",
       "1                                      E                                  10   \n",
       "2                                      W                                  51   \n",
       "3                                      W                                  32   \n",
       "4                                      E                                   9   \n",
       "5                                      E                                   5   \n",
       "6                                      E                                   1   \n",
       "7                                      E                                  58   \n",
       "8                                      E                                   2   \n",
       "9                                      W                                 127   \n",
       "10                                     E                                   5   \n",
       "11                                     E                                  28   \n",
       "12                                     W                                   3   \n",
       "13                                     W                                  10   \n",
       "14                                     E                                 166   \n",
       "15                                     E                                   5   \n",
       "16                                     E                                   5   \n",
       "17                                     E                                   5   \n",
       "18                                     E                                  16   \n",
       "19                                     W                                   8   \n",
       "\n",
       "                              Mag  [+]                    Region name  [+]  \\\n",
       "   1  2  3  4  5  6  7  8  9  10  ›  » 1  2  3  4  5  6  7  8  9  10  ›  »   \n",
       "0                                   ML                                 2.0   \n",
       "1                                    M                                 3.1   \n",
       "2                                    M                                 2.5   \n",
       "3                                    M                                 2.7   \n",
       "4                                   ML                                 2.1   \n",
       "5                                   ML                                 2.9   \n",
       "6                                   ML                                 2.5   \n",
       "7                                    M                                 3.0   \n",
       "8                                   ML                                 3.4   \n",
       "9                                    M                                 2.6   \n",
       "10                                   M                                 2.8   \n",
       "11                                   M                                 3.6   \n",
       "12                                  ML                                 2.5   \n",
       "13                                  ML                                 3.0   \n",
       "14                                  mb                                 4.4   \n",
       "15                                  ML                                 2.4   \n",
       "16                                  ML                                 3.7   \n",
       "17                                  ML                                 3.9   \n",
       "18                                  ML                                 2.7   \n",
       "19                                  Ml                                 2.0   \n",
       "\n",
       "                      Last update  [-]                 Unnamed: 12_level_0  \n",
       "   1  2  3  4  5  6  7  8  9  10  ›  » 1  2  3  4  5  6  7  8  9  10  ›  »  \n",
       "0                       CENTRAL TURKEY                    2023-03-21 10:59  \n",
       "1                       CENTRAL TURKEY                    2023-03-21 10:51  \n",
       "2                       ATACAMA, CHILE                    2023-03-21 10:48  \n",
       "3              NEAR COAST OF NICARAGUA                    2023-03-21 10:50  \n",
       "4                       CENTRAL TURKEY                    2023-03-21 10:36  \n",
       "5                       CENTRAL TURKEY                    2023-03-21 10:26  \n",
       "6                       CENTRAL TURKEY                    2023-03-21 10:21  \n",
       "7          SOUTH ISLAND OF NEW ZEALAND                    2023-03-21 10:15  \n",
       "8                    NORTHWESTERN IRAN                    2023-03-21 10:22  \n",
       "9                   ANTOFAGASTA, CHILE                    2023-03-21 10:13  \n",
       "10                      CENTRAL TURKEY                    2023-03-21 10:10  \n",
       "11         SOUTHERN SUMATRA, INDONESIA                    2023-03-21 09:25  \n",
       "12                 STRAIT OF GIBRALTAR                    2023-03-21 09:23  \n",
       "13                    ALASKA PENINSULA                    2023-03-21 09:27  \n",
       "14     KEPULAUAN BARAT DAYA, INDONESIA                    2023-03-21 10:22  \n",
       "15                      CENTRAL TURKEY                    2023-03-21 08:57  \n",
       "16                      CENTRAL TURKEY                    2023-03-21 10:23  \n",
       "17                      CENTRAL TURKEY                    2023-03-21 10:24  \n",
       "18                             ROMANIA                    2023-03-21 10:24  \n",
       "19             BAJA CALIFORNIA, MEXICO                    2023-03-21 08:17  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Table #3 as there are several tables inside this website\n",
    "table_eq = soup6.find_all('table')[3]\n",
    "#Slicing upto 20 results as requested in lab:\n",
    "df = pd.read_html(table_eq.prettify())[0][:20]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Browser -> Not a secure website'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\"Browser -> Not a secure website\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req7 = requests.get(url7)\n",
    "soup7 = BeautifulSoup(req7.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Polski',\n",
       " 'العربية',\n",
       " 'Deutsch',\n",
       " 'English',\n",
       " 'Español',\n",
       " 'Français',\n",
       " 'Italiano',\n",
       " 'مصرى',\n",
       " 'Nederlands',\n",
       " '日本語',\n",
       " 'Português',\n",
       " 'Sinugboanong Binisaya',\n",
       " 'Svenska',\n",
       " 'Українська',\n",
       " 'Tiếng Việt',\n",
       " 'Winaray',\n",
       " '中文',\n",
       " 'Русский',\n",
       " 'Afrikaans',\n",
       " 'Asturianu',\n",
       " 'Azərbaycanca',\n",
       " 'Български',\n",
       " 'Bân-lâm-gú / Hō-ló-oē',\n",
       " 'বাংলা',\n",
       " 'Беларуская',\n",
       " 'Català',\n",
       " 'Čeština',\n",
       " 'Cymraeg',\n",
       " 'Dansk',\n",
       " 'Eesti',\n",
       " 'Ελληνικά',\n",
       " 'Esperanto',\n",
       " 'Euskara',\n",
       " 'فارسی',\n",
       " 'Galego',\n",
       " '한국어',\n",
       " 'हिन्दी',\n",
       " 'Hrvatski',\n",
       " 'Bahasa Indonesia',\n",
       " 'עברית',\n",
       " 'ქართული',\n",
       " 'Latina',\n",
       " 'Latviešu',\n",
       " 'Lietuvių',\n",
       " 'Magyar',\n",
       " 'Македонски',\n",
       " 'Bahasa Melayu',\n",
       " 'Bahaso Minangkabau',\n",
       " 'Norskbokmålnynorsk',\n",
       " 'bokmål',\n",
       " 'nynorsk',\n",
       " 'Нохчийн',\n",
       " 'Oʻzbekcha / Ўзбекча',\n",
       " 'Қазақша / Qazaqşa / قازاقشا',\n",
       " 'Română',\n",
       " 'Simple English',\n",
       " 'Slovenčina',\n",
       " 'Slovenščina',\n",
       " 'Српски / Srpski',\n",
       " 'Srpskohrvatski / Српскохрватски',\n",
       " 'Suomi',\n",
       " 'தமிழ்',\n",
       " 'Татарча / Tatarça',\n",
       " 'ภาษาไทย',\n",
       " 'Тоҷикӣ',\n",
       " 'تۆرکجه',\n",
       " 'Türkçe',\n",
       " 'اردو',\n",
       " '粵語',\n",
       " 'Հայերեն',\n",
       " 'မြန်မာဘာသာ',\n",
       " 'Bahsa Acèh',\n",
       " 'Alemannisch',\n",
       " 'አማርኛ',\n",
       " 'Aragonés',\n",
       " 'Արեւմտահայերէն',\n",
       " 'Basa Bali',\n",
       " 'Bahasa Banjar',\n",
       " 'Basa Banyumasan',\n",
       " 'Башҡортса',\n",
       " 'Беларуская (Тарашкевіца)',\n",
       " 'Bikol Central',\n",
       " 'বিষ্ণুপ্রিয়া মণিপুরী',\n",
       " 'Boarisch',\n",
       " 'Bosanski',\n",
       " 'Brezhoneg',\n",
       " 'Чӑвашла',\n",
       " 'Diné Bizaad',\n",
       " 'Emigliàn–Rumagnòl',\n",
       " 'Fiji Hindi',\n",
       " 'Føroyskt',\n",
       " 'Frysk',\n",
       " 'Gaeilge',\n",
       " 'Gàidhlig',\n",
       " 'ગુજરાતી',\n",
       " 'Hak-kâ-ngî / 客家語',\n",
       " 'Hausa',\n",
       " 'Hornjoserbsce',\n",
       " 'Ido',\n",
       " 'Igbo',\n",
       " 'Ilokano',\n",
       " 'Interlingua',\n",
       " 'Interlingue',\n",
       " 'Ирон',\n",
       " 'Íslenska',\n",
       " 'Jawa',\n",
       " 'ಕನ್ನಡ',\n",
       " 'Kreyòl Ayisyen',\n",
       " 'Kurdî / كوردی',\n",
       " 'کوردیی ناوەندی',\n",
       " 'Кыргызча',\n",
       " 'Кырык мары',\n",
       " 'Lëtzebuergesch',\n",
       " 'Lìgure',\n",
       " 'Limburgs',\n",
       " 'Lombard',\n",
       " 'मैथिली',\n",
       " 'Malagasy',\n",
       " 'മലയാളം',\n",
       " '文言',\n",
       " 'मराठी',\n",
       " 'მარგალური',\n",
       " 'مازِرونی',\n",
       " 'Mìng-dĕ̤ng-ngṳ̄ / 閩東語',\n",
       " 'Монгол',\n",
       " 'Napulitano',\n",
       " 'नेपाल भाषा',\n",
       " 'नेपाली',\n",
       " 'Nordfriisk',\n",
       " 'Occitan',\n",
       " 'Олык марий',\n",
       " 'ଓଡି଼ଆ',\n",
       " 'অসমীযা়',\n",
       " 'ਪੰਜਾਬੀ (ਗੁਰਮੁਖੀ)',\n",
       " 'پنجابی (شاہ مکھی)',\n",
       " 'پښتو',\n",
       " 'Piemontèis',\n",
       " 'Plattdüütsch',\n",
       " 'Qırımtatarca',\n",
       " 'Runa Simi',\n",
       " 'संस्कृतम्',\n",
       " 'Саха Тыла',\n",
       " 'Scots',\n",
       " 'ChiShona',\n",
       " 'Shqip',\n",
       " 'Sicilianu',\n",
       " 'සිංහල',\n",
       " 'سنڌي',\n",
       " 'Ślůnski',\n",
       " 'Basa Sunda',\n",
       " 'Kiswahili',\n",
       " 'Tagalog',\n",
       " 'ၽႃႇသႃႇတႆး',\n",
       " 'తెలుగు',\n",
       " 'ᨅᨔ ᨕᨙᨁᨗ / Basa Ugi',\n",
       " 'Vèneto',\n",
       " 'Volapük',\n",
       " 'Walon',\n",
       " '吴语',\n",
       " 'ייִדיש',\n",
       " 'Yorùbá',\n",
       " 'Zazaki',\n",
       " 'Žemaitėška',\n",
       " 'isiZulu',\n",
       " 'Dzhudezmo / לאדינו',\n",
       " 'Адыгэбзэ',\n",
       " 'Ænglisc',\n",
       " 'Anarâškielâ',\n",
       " 'аԥсшәа',\n",
       " 'Armãneashce',\n",
       " 'Arpitan',\n",
       " 'ܐܬܘܪܝܐ',\n",
       " 'Avañe’ẽ',\n",
       " 'Авар',\n",
       " 'Aymar',\n",
       " 'भोजपुरी',\n",
       " 'Bislama',\n",
       " 'བོད་ཡིག',\n",
       " 'Буряад',\n",
       " 'Chavacano de Zamboanga',\n",
       " 'Chichewa',\n",
       " 'Corsu',\n",
       " 'Vahcuengh / 話僮',\n",
       " 'Dagbanli',\n",
       " 'الدارجة',\n",
       " 'Davvisámegiella',\n",
       " 'Deitsch',\n",
       " 'ދިވެހިބަސް',\n",
       " 'Dolnoserbski',\n",
       " 'Эрзянь',\n",
       " 'Estremeñu',\n",
       " 'Furlan',\n",
       " 'Gaelg',\n",
       " 'Gagauz',\n",
       " 'ГӀалгӀай',\n",
       " 'Gĩkũyũ',\n",
       " 'گیلکی',\n",
       " '赣语 / 贛語',\n",
       " 'Gungbe',\n",
       " 'Хальмг',\n",
       " 'ʻŌlelo Hawaiʻi',\n",
       " 'Ikinyarwanda',\n",
       " 'Kabɩyɛ',\n",
       " 'Kapampangan',\n",
       " 'Kaszëbsczi',\n",
       " 'Kernewek',\n",
       " 'ភាសាខ្មែរ',\n",
       " 'Коми',\n",
       " 'Перем коми',\n",
       " 'Kongo',\n",
       " 'कोंकणी / Konknni',\n",
       " 'كٲشُر',\n",
       " 'Kriyòl Gwiyannen',\n",
       " 'ພາສາລາວ',\n",
       " 'Лакку',\n",
       " 'Latgaļu',\n",
       " 'Лезги',\n",
       " 'Li Niha',\n",
       " 'Lingála',\n",
       " 'lojban',\n",
       " 'Luganda',\n",
       " 'Malti',\n",
       " 'Māori',\n",
       " 'Twi',\n",
       " 'Mirandés',\n",
       " 'Мокшень',\n",
       " 'ဘာသာ မန်',\n",
       " 'ߒߞߏ',\n",
       " 'Na Vosa Vaka-Viti',\n",
       " 'Nāhuatlahtōlli',\n",
       " 'Dorerin Naoero',\n",
       " 'Nedersaksisch',\n",
       " 'Nouormand / Normaund',\n",
       " 'Novial',\n",
       " 'Afaan Oromoo',\n",
       " 'ပအိုဝ်ႏဘာႏသာႏ',\n",
       " 'पालि',\n",
       " 'Pangasinán',\n",
       " 'Papiamentu',\n",
       " 'Pfälzisch',\n",
       " 'Picard',\n",
       " 'Къарачай–малкъар',\n",
       " 'Qaraqalpaqsha',\n",
       " 'Ripoarisch',\n",
       " 'Rumantsch',\n",
       " 'Русиньскый',\n",
       " 'Gagana Sāmoa',\n",
       " 'ᱥᱟᱱᱛᱟᱲᱤ',\n",
       " 'سرائیکی',\n",
       " 'Sardu',\n",
       " 'Seediq',\n",
       " 'Seeltersk',\n",
       " 'Sesotho sa Leboa',\n",
       " 'Setswana',\n",
       " 'Словѣ́ньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟ',\n",
       " 'Soomaaliga',\n",
       " 'Sranantongo',\n",
       " 'Taqbaylit',\n",
       " 'Tarandíne',\n",
       " 'Tetun',\n",
       " 'Tok Pisin',\n",
       " 'faka Tonga',\n",
       " 'ᏣᎳᎩ',\n",
       " 'chiTumbuka',\n",
       " 'Türkmençe',\n",
       " 'Тыва дыл',\n",
       " 'Удмурт',\n",
       " 'ئۇيغۇرچه',\n",
       " 'Vepsän',\n",
       " 'Võro',\n",
       " 'West-Vlams',\n",
       " 'Wolof',\n",
       " 'isiXhosa',\n",
       " 'Zeêuws',\n",
       " 'Reo tahiti',\n",
       " 'Akan',\n",
       " 'Bamanankan',\n",
       " 'Chamoru',\n",
       " 'Eʋegbe',\n",
       " 'Fulfulde',\n",
       " '𐌲𐌿𐍄𐌹𐍃𐌺',\n",
       " 'ᐃᓄᒃᑎᑐᑦ / Inuktitut',\n",
       " 'Iñupiak',\n",
       " 'Kalaallisut',\n",
       " 'Madhurâ',\n",
       " 'Nēhiyawēwin / ᓀᐦᐃᔭᐍᐏᐣ',\n",
       " 'Norfuk / Pitkern',\n",
       " 'Pangcah',\n",
       " 'pinayuanan',\n",
       " 'Ποντιακά',\n",
       " 'རྫོང་ཁ',\n",
       " 'romani čhib',\n",
       " 'Ikirundi',\n",
       " 'Sängö',\n",
       " 'Sesotho',\n",
       " 'SiSwati',\n",
       " 'ትግርኛ',\n",
       " 'Thuɔŋjäŋ',\n",
       " 'Tsėhesenėstsestotse',\n",
       " 'Xitsonga',\n",
       " 'Tyap',\n",
       " 'Tshivenḓa']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang = soup7.find_all('div', attrs ={'class': 'lang-list-content'})[0].find_all('li')\n",
    "lang_list = []\n",
    "\n",
    "for i in all_lang:\n",
    "    if i.getText():\n",
    "        lang_list.append(i.getText())\n",
    "\n",
    "lang_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "req8 = requests.get(url8)\n",
    "soup8 = BeautifulSoup(req8.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data= soup8.find_all('h3', attrs ={'class':'govuk-heading-s dgu-topics__heading'})\n",
    "data_repo = []\n",
    "\n",
    "for i in all_data:\n",
    "    if i.getText():\n",
    "        data_repo.append(i.getText())\n",
    "        \n",
    "data_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "req9 = requests.get(url9)\n",
    "soup9 = BeautifulSoup(req9.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers  (millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese  (incl.  Standard Chinese  , ...</td>\n",
       "      <td>939.0</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>485.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>380.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi  (excl.  Urdu  , and  other languages  )</td>\n",
       "      <td>345.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>236.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>147.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>123.0</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese  (incl.  Cantonese  )</td>\n",
       "      <td>86.1</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Language  \\\n",
       "0  Mandarin Chinese  (incl.  Standard Chinese  , ...   \n",
       "1                                            Spanish   \n",
       "2                                            English   \n",
       "3     Hindi  (excl.  Urdu  , and  other languages  )   \n",
       "4                                         Portuguese   \n",
       "5                                            Bengali   \n",
       "6                                            Russian   \n",
       "7                                           Japanese   \n",
       "8                  Yue Chinese  (incl.  Cantonese  )   \n",
       "9                                         Vietnamese   \n",
       "\n",
       "   Native speakers  (millions) Language family        Branch  \n",
       "0                        939.0    Sino-Tibetan       Sinitic  \n",
       "1                        485.0   Indo-European       Romance  \n",
       "2                        380.0   Indo-European      Germanic  \n",
       "3                        345.0   Indo-European    Indo-Aryan  \n",
       "4                        236.0   Indo-European       Romance  \n",
       "5                        234.0   Indo-European    Indo-Aryan  \n",
       "6                        147.0   Indo-European  Balto-Slavic  \n",
       "7                        123.0         Japonic      Japanese  \n",
       "8                         86.1    Sino-Tibetan       Sinitic  \n",
       "9                         85.0   Austroasiatic        Vietic  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_lang = soup9.find_all('table')[0]\n",
    "df = pd.read_html(table_lang.prettify())[0][:10]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url10 = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "req10 = requests.get(url10)\n",
    "soup10 = BeautifulSoup(req10.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_movies = soup10.find_all('td', attrs ={'class':'titleColumn'})\n",
    "rank = []\n",
    "movie = []\n",
    "year = []\n",
    "rating = []\n",
    "dir_ = []\n",
    "stars_ = []\n",
    "movies_repo = {\n",
    "    'Rank': rank,\n",
    "    'Title': movie,\n",
    "    'Year': year,\n",
    "    'IMDB Rating': rating,\n",
    "    'Director': dir_,\n",
    "    'Stars': stars_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from table movies we can get ranking ind.0 movie ind.1 year ind.2 from getText and by finding 'a' we can get dir and actors\n",
    "for i in table_movies:\n",
    "    if i.getText().strip().split('\\n'):\n",
    "        rank.append(i.getText().strip().split('\\n')[0])\n",
    "        movie.append(i.getText().strip().split('\\n')[1])\n",
    "        year.append(i.getText().strip().split('\\n')[2].replace('(','').replace(')',''))\n",
    "    if i.find('a').get('title').split('(dir.)'):\n",
    "        dir_.append(i.find('a').get('title').split('(dir.)')[0])\n",
    "        stars_.append(i.find('a').get('title').split('(dir.)')[1][1:])\n",
    "#df = pd.DataFrame(movies_repo)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>IMDB Rating</th>\n",
       "      <th>Director</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins, Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando, Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale, Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>El padrino (parte II)</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino, Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda, Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>246.</td>\n",
       "      <td>Criadas y señoras</td>\n",
       "      <td>2011</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Tate Taylor</td>\n",
       "      <td>Viola Davis, Emma Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>247.</td>\n",
       "      <td>Dersu Uzala (El cazador)</td>\n",
       "      <td>1975</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Akira Kurosawa</td>\n",
       "      <td>Maksim Munzuk, Yuriy Solomin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>248.</td>\n",
       "      <td>Aladdín</td>\n",
       "      <td>1992</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Ron Clements</td>\n",
       "      <td>Scott Weinger, Robin Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>249.</td>\n",
       "      <td>Bailando con lobos</td>\n",
       "      <td>1990</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Kevin Costner</td>\n",
       "      <td>Kevin Costner, Mary McDonnell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>250.</td>\n",
       "      <td>Gandhi</td>\n",
       "      <td>1982</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Richard Attenborough</td>\n",
       "      <td>Ben Kingsley, John Gielgud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                           Title  Year IMDB Rating  \\\n",
       "0      1.                 Cadena perpetua  1994         9.2   \n",
       "1      2.                      El padrino  1972         9.2   \n",
       "2      3.             El caballero oscuro  2008         9.0   \n",
       "3      4.           El padrino (parte II)  1974         9.0   \n",
       "4      5.           12 hombres sin piedad  1957         9.0   \n",
       "..    ...                             ...   ...         ...   \n",
       "245  246.               Criadas y señoras  2011         8.0   \n",
       "246  247.        Dersu Uzala (El cazador)  1975         8.0   \n",
       "247  248.                         Aladdín  1992         8.0   \n",
       "248  249.              Bailando con lobos  1990         8.0   \n",
       "249  250.                          Gandhi  1982         8.0   \n",
       "\n",
       "                  Director                           Stars  \n",
       "0          Frank Darabont      Tim Robbins, Morgan Freeman  \n",
       "1    Francis Ford Coppola         Marlon Brando, Al Pacino  \n",
       "2       Christopher Nolan     Christian Bale, Heath Ledger  \n",
       "3    Francis Ford Coppola        Al Pacino, Robert De Niro  \n",
       "4            Sidney Lumet         Henry Fonda, Lee J. Cobb  \n",
       "..                     ...                             ...  \n",
       "245           Tate Taylor          Viola Davis, Emma Stone  \n",
       "246        Akira Kurosawa     Maksim Munzuk, Yuriy Solomin  \n",
       "247          Ron Clements    Scott Weinger, Robin Williams  \n",
       "248         Kevin Costner    Kevin Costner, Mary McDonnell  \n",
       "249  Richard Attenborough       Ben Kingsley, John Gielgud  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ratings = soup10.find_all('td', attrs ={'class':'ratingColumn imdbRating'})\n",
    "for i in table_ratings:\n",
    "    if i.getText().strip():\n",
    "        rating.append(i.getText().strip())\n",
    "        \n",
    "df = pd.DataFrame(movies_repo)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
